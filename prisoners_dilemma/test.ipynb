{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from pymdp import utils\n",
    "import numpy as np\n",
    "from pymdp.agent import Agent\n",
    "from pymdp.maths import softmax, spm_log_single\n",
    "import copy\n",
    "\n",
    "num_observations = 4  # ((1, -2, 2, 0) reward levels\n",
    "num_actions = 2  # cooperate, cheat\n",
    "num_states = (4, 2)  # ((the possible combinations), (procosial, antisocial))\n",
    "# (cooperate & cooperate): ++, (cooperate & defect): +-, (defect&cooperate): -+ , (defect&defect): --\n",
    "\n",
    "num_modalities = 1\n",
    "num_factors = 2\n",
    "\n",
    "precision_prosocial = 3.0\n",
    "precision_antisocial = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymdp.maths import softmax\n",
    "from pymdp import utils\n",
    "\n",
    "def construct_A(precision_prosocial=3.0, precision_antisocial=2.0):\n",
    "    A1_prosocial = np.zeros((4, 4))\n",
    "    A1_prosocial[:, 0] = softmax(precision_prosocial * np.array([1, 0, 0, 0]))\n",
    "    A1_prosocial[:, 1] = softmax(precision_antisocial * np.array([0, 1, 0, 0]))\n",
    "    A1_prosocial[:, 2] = softmax(precision_prosocial * np.array([0, 0, 1, 0]))\n",
    "    A1_prosocial[:, 3] = softmax(precision_antisocial * np.array([0, 0, 0, 1]))\n",
    "\n",
    "    A1_antisocial = np.zeros((4, 4))\n",
    "    A1_antisocial[:, 0] = softmax(\n",
    "        precision_antisocial * np.array([1, 0, 0, 0])\n",
    "    )\n",
    "    A1_antisocial[:, 1] = softmax(precision_prosocial * np.array([0, 1, 0, 0]))\n",
    "    A1_antisocial[:, 2] = softmax(\n",
    "        precision_antisocial * np.array([0, 0, 1, 0])\n",
    "    )\n",
    "    A1_antisocial[:, 3] = softmax(precision_prosocial * np.array([0, 0, 0, 1]))\n",
    "    A = utils.obj_array(1)\n",
    "\n",
    "    A1 = np.zeros((4, 4, 2))\n",
    "    A1[:, :, 0] = A1_prosocial\n",
    "    A1[:, :, 1] = A1_antisocial\n",
    "    A[0] = A1\n",
    "    return A\n",
    "\n",
    "\n",
    "def sample_action_policy_directly(q_pi, policies, num_controls, style = \"deterministic\"):\n",
    "\n",
    "    num_factors = len(num_controls)\n",
    "\n",
    "    if style == \"deterministic\":\n",
    "        policy_idx = np.argmax(q_pi)\n",
    "    elif style == \"stochastic\":\n",
    "        policy_idx = utils.sample(q_pi)\n",
    "    \n",
    "    selected_policy = np.zeros(num_factors)\n",
    "    for factor_i in range(num_factors):\n",
    "        selected_policy[factor_i] = policies[policy_idx][0, factor_i]\n",
    "\n",
    "    return selected_policy\n",
    "\n",
    "def get_observation(action_1, action_2):\n",
    "    action_1 == int(action_1)\n",
    "    action_2 = int(action_2)\n",
    "    if action_1 == 0 and action_2 == 0:\n",
    "        return [0]\n",
    "    elif action_1 == 0 and action_2 == 1:\n",
    "        return [1]\n",
    "    elif action_1 == 1 and action_2 == 0:\n",
    "        return [2]\n",
    "    elif action_1 == 1 and action_2 == 1:\n",
    "        return [3]\n",
    "\n",
    "\n",
    "def print_A(A):\n",
    "    print(\"A1: observation of reward to reward states\")\n",
    "    print(A[0])\n",
    "    print(A[0].shape)\n",
    "    print(A[0][3, :, :])\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_B(B):\n",
    "    print(\"B1: transitions from reward states given action cooperate\")\n",
    "    print(B[0][:, :, 0])\n",
    "    print()\n",
    "    print(\"B1: transitions from reward states given action cheat\")\n",
    "    print(B[0][:, :, 1])\n",
    "\n",
    "    print(\"B2: transitions from cooperation states given action cooperate\")\n",
    "    print(B[1][:, :, 0])\n",
    "    print()\n",
    "    print(\"B2: transitions from cooperation states given action cheat\")\n",
    "    print(B[1][:, :, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The probability of being in reward states cc or dc are more likely if the neighbour is prosocial\"\"\"\n",
    "\n",
    "A = construct_A(precision_prosocial, precision_antisocial)\n",
    "\n",
    "# print_A(A)\n",
    "\n",
    "B = utils.obj_array(num_factors)\n",
    "B_1 = np.ones((4, 4, 2)) * 0.5\n",
    "B_1[2:, :, 0] = 0.0\n",
    "B_1[:2, :, 1] = 0.0\n",
    "B[0] = B_1\n",
    "\n",
    "B_2 = np.zeros((2, 2, 2))\n",
    "# B_2[:, :, 0] = np.array(\n",
    "#     [\n",
    "#         [0.8, 0.4],  # given that i cooperated, the probability p (s = prosocial | s_t=1 = prosocial)\n",
    "#         [0.2, 0.6],\n",
    "#     ] )\n",
    "\n",
    "B_2[:, :, 0] = np.array(\n",
    "    [\n",
    "        [0.5, 0.5],  # given that i cooperated, the probability p (s = prosocial | s_t=1 = prosocial)\n",
    "        [0.5, 0.5],\n",
    "    ] )\n",
    "\n",
    "B_2[:, :, 1] = np.array(\n",
    "    [\n",
    "        [0.5, 0.5],  # given that i cooperated, the probability p (s = prosocial | s_t=1 = prosocial)\n",
    "        [0.5, 0.5],\n",
    "    ] )\n",
    "\n",
    "# B_2[:, :, 1] = np.array([[0.6, 0.2], [0.4, 0.8]])\n",
    "\n",
    "B[1] = B_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" We don't allow policies [1,0] or [0,1]\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = utils.obj_array(num_modalities)\n",
    "C[0] = np.array([4, 1, 4, 3])\n",
    "lr_pb = 0.25\n",
    "\n",
    "D = utils.obj_array(num_factors)\n",
    "\n",
    "D[0] = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "D[1] = np.array([0.5, 0.5])\n",
    "\n",
    "# pB_1 = utils.dirichlet_like(B)\n",
    "\n",
    "# pB_2 = utils.dirichlet_like(B)\n",
    "\n",
    "# agent_1 = Agent(A=A, B=B, C=C, D=D, pB=pB_1, lr_pB=10, factors_to_learn=[0])\n",
    "# agent_2 = Agent(A=A, B=B, C=C, D=D, pB=pB_2, lr_pB=10, factors_to_learn=[0])\n",
    "\n",
    "# agent_1 = Agent(A=A, B=B, C=C, D=D, pB=pB_1, lr_pB=10)\n",
    "# agent_2 = Agent(A=A, B=B, C=C, D=D, pB=pB_2, lr_pB=10)\n",
    "\n",
    "agent_1 = Agent(A=A, B=B, C=C, D=D, policies = [np.array([[0,0]]), np.array([[1, 1]])])\n",
    "agent_2 = Agent(A=A, B=B, C=C, D=D, policies = [np.array([[0,0]]), np.array([[1, 1]])])\n",
    "\n",
    "\"\"\" We don't allow policies [1,0] or [0,1]\"\"\"\n",
    "# agent_1.policies = [agent_1.policies[0], agent_1.policies[3]]\n",
    "\n",
    "# agent_1.policies[1] = agent_1.policies[0]\n",
    "# agent_1.policies[2] = agent_1.policies[3]\n",
    "# agent_2.policies[1] = agent_2.policies[0]\n",
    "# agent_2.policies[2] = agent_2.policies[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_1 = [0]\n",
    "observation_2 = [0]\n",
    "actions = [\"cooperate\", \"cheat\"]\n",
    "\n",
    "qs_prev_1 = copy.deepcopy(D)\n",
    "qs_prev_2 = copy.deepcopy(D)\n",
    "\n",
    "observation_names1 = [\"cc\", \"cd\", \"dc\", \"dd\"]\n",
    "observation_names2 = [\"prosocial\", \"antisocial\"]\n",
    "\n",
    "action_names = [\"cooperate\", \"cheat\"]\n",
    "\n",
    "T = 50\n",
    "\n",
    "actions_over_time = np.zeros((T, 2))\n",
    "B_over_time = np.zeros((T, 2, 4, 2))\n",
    "q_pi_over_time = np.zeros((T, 2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "qs_1 = agent_1.infer_states(observation_1)\n",
    "qs_2 = agent_2.infer_states(observation_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pi_1, efe_1 = agent_1.infer_policies()\n",
    "q_pi_2, efe_2 = agent_2.infer_policies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_1 = sample_action_policy_directly(\n",
    "        q_pi_1, agent_1.policies, agent_1.num_controls\n",
    "    )\n",
    "action_2 = sample_action_policy_directly(\n",
    "    q_pi_2, agent_2.policies, agent_2.num_controls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_1 = action_1[1]\n",
    "action_2 = action_2[1]\n",
    "actions_over_time[t] = [action_1, action_2]\n",
    "\n",
    "observation_1 = get_observation(action_1, action_2)\n",
    "observation_2 = get_observation(action_2, action_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 100\n",
    "T_per_trial = 2\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    for t > 0\n",
    "\n",
    "        o_t = get_observation(action)\n",
    "        q_s = infer_states(o_t)\n",
    "        update_B(qs_now = q_s, qs_prev = qs_prev)\n",
    "\n",
    "        q_pi = infer_policies(q_s)\n",
    "        action = sample_action(q_pi)\n",
    "        q_s_prev = q_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = : 0\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 1\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 2\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 3\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 4\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 5\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 6\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 7\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 8\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 9\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 10\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 11\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 12\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 13\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 14\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 15\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 16\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 17\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 18\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 19\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 20\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 21\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 22\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 23\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 24\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 25\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 26\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 27\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 28\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 29\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 30\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 31\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 32\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 33\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 34\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 35\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 36\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 37\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 38\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 39\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 40\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 41\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 42\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 43\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 44\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 45\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 46\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 47\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 48\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n",
      "time = : 49\n",
      "q_pi\n",
      "[0.5 0.5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(T):\n",
    "    print(f\"time = : {t}\")\n",
    "\n",
    "    qs_1 = agent_1.infer_states(observation_1)\n",
    "    qs_2 = agent_2.infer_states(observation_2)\n",
    "\n",
    "    qB_1 = agent_1.update_B(qs_prev_1)\n",
    "    qB_2 = agent_2.update_B(qs_prev_2)\n",
    "    \"\"\"\n",
    "    print(\"observations\")\n",
    "    print(observation_1)\n",
    "    print(observation_2)\n",
    "    print(\"AGENT 1 A\")\n",
    "\n",
    "    print(A[0][int(observation_1[0]),:])\n",
    "    print(\"AGENT 2 A\")\n",
    "\n",
    "    print(A[0][int(observation_2[0]),:])\n",
    "    print(\"qs\")\n",
    "    print(qs_1)\n",
    "    print(qs_2)\n",
    "    print()\n",
    "    \"\"\"\n",
    "    q_pi_1, efe_1 = agent_1.infer_policies()\n",
    "    q_pi_2, efe_2 = agent_2.infer_policies()\n",
    "    # print(\"q_pi\")\n",
    "    # print(q_pi_1)\n",
    "    # print()\n",
    "    # action_1 = agent_1.sample_action()\n",
    "    # action_2 = agent_2.sample_action()\n",
    "    action_1 = sample_action_policy_directly(\n",
    "        q_pi_1, agent_1.policies, agent_1.num_controls, style = \"deterministic\"\n",
    "    )\n",
    "    action_2 = sample_action_policy_directly(\n",
    "        q_pi_2, agent_2.policies, agent_2.num_controls, style = \"deterministic\"\n",
    "    )\n",
    "    agent_1.action = action_1\n",
    "    agent_2.action = action_2\n",
    "\n",
    "    action_1 = action_1[1]\n",
    "    action_2 = action_2[1]\n",
    "    actions_over_time[t] = [action_1, action_2]\n",
    "\n",
    "    \n",
    "\n",
    "    observation_1 = get_observation(action_1, action_2)\n",
    "    observation_2 = get_observation(action_2, action_1)\n",
    "    \"\"\"\n",
    "    print(\"AGENT 1 B\")\n",
    "    print(agent_1.B[0][:,int(observation_1[0]),0])\n",
    "    print(agent_1.B[0][:,int(observation_1[0]),1])\n",
    "    print(\"AGENT 2 B\")\n",
    "    print(agent_2.B[0][:,int(observation_2[0]),0])\n",
    "    print(agent_2.B[0][:,int(observation_2[0]),1])\n",
    "    \"\"\"\n",
    "    # B_over_time[t, 0, :, 0] = agent_1.B[0][:, int(observation_1[0]), 0]\n",
    "    # B_over_time[t, 1, :, 0] = agent_1.B[0][:, int(observation_1[0]), 1]\n",
    "    # B_over_time[t, 0, :, 1] = agent_2.B[0][:, int(observation_2[0]), 0]\n",
    "    # B_over_time[t, 1, :, 1] = agent_2.B[0][:, int(observation_2[0]), 1]\n",
    "\n",
    "    # B1_over_time[t,:,:,:,0] = agent_1.B[0]\n",
    "    # B1_over_time[t,:,:,:,0] = agent_2.B[0]\n",
    "\n",
    "    # B2_over_time[t,:,:,:,0] = agent_1.B[1]\n",
    "    # B2_over_time[t,:,:,:,0] = agent_2.B[1]\n",
    "\n",
    "    # q_pi_over_time[t, :, 0] = [q_pi_1[0], q_pi_1[3]]\n",
    "    # q_pi_over_time[t, :, 1] = [q_pi_2[0], q_pi_2[3]]\n",
    "\n",
    "    # num_factors = len(pB_1)\n",
    "\n",
    "    # qB = copy.deepcopy(pB_1)\n",
    "\n",
    "    # qB_1 = agent_1.update_B(qs_prev_1)\n",
    "    # qB_2 = agent_2.update_B(qs_prev_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Actions over time for A precisions 2.0, 2.0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(actions_over_time.T, cmap=\"gray\")\n",
    "\n",
    "plt.title(\n",
    "    f\"Actions over time for A precisions {precision_prosocial}, {precision_antisocial}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax.set_yticks([0, 1], labels=[\"Agent 1\", \"Agent 2\"])\n",
    "\n",
    "plt.savefig(\"actions_over_time2\")\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cmap0 = LinearSegmentedColormap.from_list(\"\", [\"white\", \"darkblue\"])\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(B_over_time[0, :, :, 0], cmap=cmap0, vmin=0, vmax=1)\n",
    "plt.yticks([0, 1], labels=[\"Cooperate\", \"Defect\"])\n",
    "plt.xticks([0, 1, 2, 3], labels=[\"CC\", \"CD\", \"DC\", \"DD\"])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(B_over_time[10, :, :, 0], cmap=cmap0, vmin=0, vmax=1)\n",
    "plt.xticks([0, 1, 2, 3], labels=[\"CC\", \"CD\", \"DC\", \"DD\"])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(B_over_time[49, :, :, 0], cmap=cmap0, vmin=0, vmax=1)\n",
    "plt.xticks([0, 1, 2, 3], labels=[\"CC\", \"CD\", \"DC\", \"DD\"])\n",
    "\n",
    "plt.savefig(\"B matrix over time agent 1 2\")\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(B_over_time[0, :, :, 1], cmap=cmap0, vmin=0, vmax=1)\n",
    "plt.yticks([0, 1], labels=[\"Cooperate\", \"Defect\"])\n",
    "plt.xticks([0, 1, 2, 3], labels=[\"CC\", \"CD\", \"DC\", \"DD\"])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(B_over_time[10, :, :, 1], cmap=cmap0, vmin=0, vmax=1)\n",
    "plt.xticks([0, 1, 2, 3], labels=[\"CC\", \"CD\", \"DC\", \"DD\"])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(B_over_time[49, :, :, 1], cmap=cmap0, vmin=0, vmax=1)\n",
    "plt.xticks([0, 1, 2, 3], labels=[\"CC\", \"CD\", \"DC\", \"DD\"])\n",
    "\n",
    "plt.savefig(\"B matrix over time agent 2 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('prisonnersdilemma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f184328a1937fdda92c81ec5fdc48fd1bac9cf9751349570b13e2012c182008"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
